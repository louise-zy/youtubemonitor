import pandas as pd
import requests
from bs4 import BeautifulSoup
import hashlib
import json
import time
from datetime import datetime, timedelta
import smtplib
import email.mime.text
import email.mime.multipart
import schedule
import logging
from urllib.parse import urljoin, urlparse
import os
import sqlite3
from dataclasses import dataclass
from typing import List, Dict, Optional
import re
from utils.dingtalk import DingTalkClient
from utils.ai import AIContentProcessor

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('blogger_monitor.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# AI APIç›¸å…³ï¼ˆå¯é€‰ï¼‰
try:
    import openai
    AI_API_AVAILABLE = True
except ImportError:
    AI_API_AVAILABLE = False
    logging.warning("OpenAIåº“æœªå®‰è£…ï¼ŒAIæ‘˜è¦åŠŸèƒ½ä¸å¯ç”¨")

@dataclass
class BloggerInfo:
    """åšä¸»ä¿¡æ¯æ•°æ®ç±»"""
    name: str
    url: str
    description: str = ""
    platform: str = ""
    last_hash: str = ""
    last_check: str = ""
    last_update: str = ""

class DatabaseManager:
    """æ•°æ®åº“ç®¡ç†å™¨"""
    def __init__(self, db_file: str = "blogger_monitor.db"):
        self.db_file = db_file
        # å¼ºåˆ¶æ£€æŸ¥æ•°æ®åº“ç»“æ„ï¼Œå¦‚æœæœ‰é—®é¢˜å°±é‡æ–°åˆ›å»º
        self._check_and_fix_database()
        self._init_database()
        self._upgrade_database()  # å¼ºåˆ¶æ£€æŸ¥å¹¶å‡çº§æ•°æ®åº“
    
    def _check_and_fix_database(self):
        """æ£€æŸ¥å¹¶ä¿®å¤æ•°æ®åº“ç»“æ„é—®é¢˜"""
        if not os.path.exists(self.db_file):
            return  # æ•°æ®åº“ä¸å­˜åœ¨ï¼Œå°†ä¼šè¢«åˆ›å»º
            
        try:
            conn = sqlite3.connect(self.db_file)
            cursor = conn.cursor()
            
            # æ£€æŸ¥ updates è¡¨æ˜¯å¦å­˜åœ¨
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='updates'")
            table_exists = cursor.fetchone()
            
            if not table_exists:
                logging.info("updates è¡¨ä¸å­˜åœ¨ï¼Œå°†åœ¨ _init_database ä¸­åˆ›å»º")
                conn.close()
                return
            
            # æ£€æŸ¥ updates è¡¨æ˜¯å¦æœ‰ created_at åˆ—
            cursor.execute("PRAGMA table_info(updates)")
            columns = cursor.fetchall()
            column_names = [col[1] for col in columns]
            
            logging.info(f"å½“å‰ updates è¡¨çš„åˆ—: {column_names}")
            
            if 'created_at' not in column_names:
                logging.warning("æ£€æµ‹åˆ°æ•°æ®åº“ç»“æ„é—®é¢˜ï¼šç¼ºå°‘ created_at åˆ—")
                logging.info("æ­£åœ¨æ·»åŠ  created_at åˆ—...")
                cursor.execute("ALTER TABLE updates ADD COLUMN created_at TEXT")
                conn.commit()
                logging.info("âœ… å·²æˆåŠŸæ·»åŠ  created_at åˆ—")
            else:
                logging.info("æ•°æ®åº“ç»“æ„æ­£å¸¸")
                
            conn.close()
                
        except Exception as e:
            logging.error(f"æ£€æŸ¥æ•°æ®åº“æ—¶å‡ºé”™: {e}")
            # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œå¤‡ä»½å¹¶é‡æ–°åˆ›å»ºæ•°æ®åº“
            try:
                backup_name = f"{self.db_file}.backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                os.rename(self.db_file, backup_name)
                logging.info(f"æ•°æ®åº“æ£€æŸ¥å¤±è´¥ï¼Œå·²å¤‡ä»½ä¸º: {backup_name}")
            except:
                pass
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨"""
        conn = sqlite3.connect(self.db_file)
        cursor = conn.cursor()
        
        # åˆ›å»ºåšä¸»è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS bloggers (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT NOT NULL,
                url TEXT UNIQUE NOT NULL,
                description TEXT,
                platform TEXT,
                last_hash TEXT,
                last_check TEXT,
                last_update TEXT
            )
        ''')
        
        # åˆ›å»ºæ›´æ–°è®°å½•è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS updates (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                blogger_id INTEGER,
                title TEXT,
                content TEXT,
                links TEXT,
                content_hash TEXT,
                created_at TEXT,
                FOREIGN KEY (blogger_id) REFERENCES bloggers (id)
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def _upgrade_database(self):
        """å‡çº§æ•°æ®åº“ç»“æ„"""
        conn = sqlite3.connect(self.db_file)
        cursor = conn.cursor()
        
        # æ£€æŸ¥å¹¶æ·»åŠ ç¼ºå¤±çš„åˆ—ï¼ˆç”¨äºæ•°æ®åº“å‡çº§ï¼‰
        try:
            # è·å– updates è¡¨çš„åˆ—ä¿¡æ¯
            cursor.execute("PRAGMA table_info(updates)")
            columns = cursor.fetchall()
            column_names = [col[1] for col in columns]
            
            if 'created_at' not in column_names:
                logging.info("å‡çº§æ•°æ®åº“ï¼šæ·»åŠ  created_at åˆ—")
                cursor.execute("ALTER TABLE updates ADD COLUMN created_at TEXT")
                conn.commit()
                logging.info("âœ… æ•°æ®åº“å‡çº§å®Œæˆï¼šå·²æ·»åŠ  created_at åˆ—")
            else:
                logging.info("æ•°æ®åº“å·²æ˜¯æœ€æ–°ç‰ˆæœ¬")
                
        except Exception as e:
            logging.error(f"æ•°æ®åº“å‡çº§å¤±è´¥: {e}")
        
        conn.close()
    
    def save_blogger(self, blogger: BloggerInfo) -> int:
        """ä¿å­˜æˆ–æ›´æ–°åšä¸»ä¿¡æ¯"""
        conn = sqlite3.connect(self.db_file)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR REPLACE INTO bloggers 
            (name, url, description, platform, last_hash, last_check, last_update)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (blogger.name, blogger.url, blogger.description, blogger.platform,
              blogger.last_hash, blogger.last_check, blogger.last_update))
        
        blogger_id = cursor.lastrowid
        conn.commit()
        conn.close()
        return blogger_id
    
    def get_blogger_by_url(self, url: str) -> Optional[BloggerInfo]:
        """æ ¹æ®URLè·å–åšä¸»ä¿¡æ¯"""
        conn = sqlite3.connect(self.db_file)
        cursor = conn.cursor()
        
        cursor.execute('SELECT * FROM bloggers WHERE url = ?', (url,))
        row = cursor.fetchone()
        conn.close()
        
        if row:
            return BloggerInfo(
                name=row[1], url=row[2], description=row[3], platform=row[4],
                last_hash=row[5], last_check=row[6], last_update=row[7]
            )
        return None
    
    def save_update(self, blogger_id: int, title: str, content: str, links: List[Dict], content_hash: str):
        """ä¿å­˜æ›´æ–°è®°å½•"""
        conn = sqlite3.connect(self.db_file)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO updates (blogger_id, title, content, links, content_hash, created_at)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (blogger_id, title, content, json.dumps(links), content_hash, datetime.now().isoformat()))
        
        conn.commit()
        conn.close()

class ContentExtractor:
    """å†…å®¹æå–å™¨ï¼Œé’ˆå¯¹ä¸åŒç½‘ç«™ç±»å‹æå–å†…å®¹"""
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
    
    def extract_content(self, url: str) -> Dict:
        """æå–ç½‘é¡µå†…å®¹"""
        try:
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            soup = BeautifulSoup(response.text, 'html.parser')
            
            if 'substack.com' in url:
                return self._extract_substack(soup, url)
            elif 'paulgraham.com' in url:
                return self._extract_paulgraham(soup, url)
            elif 'bloomberg.com' in url:
                return self._extract_bloomberg(soup, url)
            elif 'wsj.com' in url:
                return self._extract_wsj(soup, url)
            else:
                return self._extract_generic(soup, url)
        except Exception as e:
            logging.error(f"æå–å†…å®¹å¤±è´¥ {url}: {e}")
            return {'title': '', 'content': '', 'links': [], 'error': str(e)}
    
    def _extract_substack(self, soup: BeautifulSoup, url: str) -> Dict:
        """æå–Substackå†…å®¹"""
        title = soup.find('h1', class_='post-title') or soup.find('h1')
        title = title.get_text().strip() if title else "æ— æ ‡é¢˜"
        
        content_div = soup.find('div', class_='available-content') or soup.find('div', class_='body')
        content = content_div.get_text().strip() if content_div else "æ— å†…å®¹"
        
        links = []
        for link in soup.find_all('a', href=True):
            if link.get('href').startswith('http'):
                links.append({'title': link.get_text().strip(), 'link': link.get('href')})
        
        content_hash = hashlib.md5((title + content).encode()).hexdigest()
        return {'title': title, 'content': content, 'links': links, 'hash': content_hash}
    
    def _extract_paulgraham(self, soup: BeautifulSoup, url: str) -> Dict:
        """æå–Paul Grahamç½‘ç«™å†…å®¹"""
        title = soup.find('title')
        title = title.get_text().strip() if title else "æ— æ ‡é¢˜"
        
        # Paul Grahamçš„æ–‡ç« é€šå¸¸åœ¨bodyçš„ç›´æ¥æ–‡æœ¬ä¸­
        content = soup.get_text().strip()
        
        links = []
        for link in soup.find_all('a', href=True):
            if link.get('href').startswith('http'):
                links.append({'title': link.get_text().strip(), 'link': link.get('href')})
        
        content_hash = hashlib.md5((title + content).encode()).hexdigest()
        return {'title': title, 'content': content, 'links': links, 'hash': content_hash}
    
    def _extract_bloomberg(self, soup: BeautifulSoup, url: str) -> Dict:
        """æå–Bloombergå†…å®¹"""
        title = soup.find('h1') or soup.find('title')
        title = title.get_text().strip() if title else "æ— æ ‡é¢˜"
        
        content_div = soup.find('div', class_='body-content') or soup.find('article')
        content = content_div.get_text().strip() if content_div else "æ— å†…å®¹"
        
        links = []
        for link in soup.find_all('a', href=True):
            if link.get('href').startswith('http'):
                links.append({'title': link.get_text().strip(), 'link': link.get('href')})
        
        content_hash = hashlib.md5((title + content).encode()).hexdigest()
        return {'title': title, 'content': content, 'links': links, 'hash': content_hash}
    
    def _extract_wsj(self, soup: BeautifulSoup, url: str) -> Dict:
        """æå–WSJå†…å®¹"""
        title = soup.find('h1') or soup.find('title')
        title = title.get_text().strip() if title else "æ— æ ‡é¢˜"
        
        content_div = soup.find('div', class_='article-content') or soup.find('article')
        content = content_div.get_text().strip() if content_div else "æ— å†…å®¹"
        
        links = []
        for link in soup.find_all('a', href=True):
            if link.get('href').startswith('http'):
                links.append({'title': link.get_text().strip(), 'link': link.get('href')})
        
        content_hash = hashlib.md5((title + content).encode()).hexdigest()
        return {'title': title, 'content': content, 'links': links, 'hash': content_hash}
    
    def _extract_generic(self, soup: BeautifulSoup, url: str) -> Dict:
        """é€šç”¨å†…å®¹æå–"""
        title = soup.find('h1') or soup.find('title')
        title = title.get_text().strip() if title else "æ— æ ‡é¢˜"
        
        # å°è¯•å¤šç§å¸¸è§çš„å†…å®¹å®¹å™¨
        content_selectors = [
            'article', '.content', '.post-content', '.entry-content',
            '.article-body', '.post-body', 'main', '.main-content'
        ]
        
        content = ""
        for selector in content_selectors:
            content_div = soup.select_one(selector)
            if content_div:
                content = content_div.get_text().strip()
                break
        
        if not content:
            content = soup.get_text().strip()
        
        links = []
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            if href and (href.startswith('http') or href.startswith('/')):
                if href.startswith('/'):
                    href = urljoin(url, href)
                links.append({'title': link.get_text().strip(), 'link': href})
        
        content_hash = hashlib.md5((title + content).encode()).hexdigest()
        return {'title': title, 'content': content, 'links': links, 'hash': content_hash}

class NotificationManager:
    """é€šçŸ¥ç®¡ç†å™¨"""
    def __init__(self, config: Dict):
        self.config = config
    
    def send_email_notification(self, updates: List[Dict]):
        """å‘é€é‚®ä»¶é€šçŸ¥"""
        try:
            smtp_server = self.config.get('smtp_server', 'smtp.gmail.com')
            smtp_port = self.config.get('smtp_port', 587)
            sender_email = self.config.get('sender_email', '')
            sender_password = self.config.get('sender_password', '')
            recipient_email = self.config.get('recipient_email', '')
            
            if not all([sender_email, sender_password, recipient_email]):
                logging.warning("é‚®ä»¶é…ç½®ä¸å®Œæ•´ï¼Œè·³è¿‡é‚®ä»¶å‘é€")
                return
            
            msg = email.mime.multipart.MIMEMultipart()
            msg['From'] = sender_email
            msg['To'] = recipient_email
            msg['Subject'] = f"åšä¸»æ›´æ–°é€šçŸ¥ - {len(updates)}ä¸ªæ–°æ›´æ–°"
            
            body = "å‘ç°ä»¥ä¸‹åšä¸»æ›´æ–°ï¼š\n\n"
            for update in updates:
                body += f"åšä¸»ï¼š{update['blogger_name']}\n"
                body += f"æ ‡é¢˜ï¼š{update['title']}\n"
                body += f"é“¾æ¥ï¼š{update['url']}\n"
                body += f"æ›´æ–°æ—¶é—´ï¼š{update['update_time']}\n"
                if update.get('summary'):
                    body += f"æ‘˜è¦ï¼š{update['summary']}\n"
                body += "\n" + "="*50 + "\n\n"
            
            msg.attach(email.mime.text.MIMEText(body, 'plain', 'utf-8'))
            
            server = smtplib.SMTP(smtp_server, smtp_port)
            server.starttls()
            server.login(sender_email, sender_password)
            server.send_message(msg)
            server.quit()
            
            logging.info(f"é‚®ä»¶é€šçŸ¥å‘é€æˆåŠŸï¼ŒåŒ…å«{len(updates)}ä¸ªæ›´æ–°")
            
        except Exception as e:
            logging.error(f"å‘é€é‚®ä»¶é€šçŸ¥å¤±è´¥: {e}")
    
    def save_to_file(self, updates: List[Dict]):
        """ä¿å­˜æ›´æ–°åˆ°æ–‡ä»¶"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"updates_{timestamp}.txt"
            
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(f"åšä¸»æ›´æ–°æŠ¥å‘Š - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write("="*60 + "\n\n")
                
                for update in updates:
                    f.write(f"åšä¸»ï¼š{update['blogger_name']}\n")
                    f.write(f"æ›´æ–°æ—¶é—´ï¼š{update['update_time']}\n")
                    f.write(f"æ ‡é¢˜ï¼š{update['title']}\n")
                    f.write(f"é“¾æ¥ï¼š{update['url']}\n\n")
                    
                    if update.get('summary'):
                        f.write(f"æ‘˜è¦ï¼š\n{update['summary']}\n\n")
                    
                    if update.get('outline'):
                        f.write(f"å¤§çº²ï¼š\n{update['outline']}\n\n")
                    
                    if update.get('content'):
                        content = update['content'][:1000] + "..." if len(update['content']) > 1000 else update['content']
                        f.write(f"å†…å®¹é¢„è§ˆï¼š\n{content}\n\n")
                    
                    if update.get('links'):
                        f.write("ç›¸å…³é“¾æ¥ï¼š\n")
                        for link in update['links'][:5]:
                            f.write(f"- {link.get('title', 'é“¾æ¥')}: {link.get('link', '')}\n")
                        f.write("\n")
                    
                    f.write("="*60 + "\n\n")
            
            logging.info(f"æ›´æ–°å·²ä¿å­˜åˆ°æ–‡ä»¶: {filename}")
            
        except Exception as e:
            logging.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")

class DingTalkNotifier(DingTalkClient):
    """é’‰é’‰æ¨é€é€šçŸ¥å™¨"""
    
    def send_message(self, updates: List[Dict], at_all: bool = False, at_mobiles: List[str] = None) -> bool:
        """å‘é€é’‰é’‰Markdownæ¶ˆæ¯ï¼ˆæ±‡æ€»æ¨¡å¼ï¼‰"""
        if not updates:
            return True
            
        content = "## ğŸ“° åšä¸»æ›´æ–°æ¨é€\n\n"
        for u in updates:
            content += f"### {u.get('blogger_name','æœªçŸ¥')}\n\n"
            content += f"**ğŸ•’ æ›´æ–°æ—¶é—´ï¼š** {u.get('update_time','')}\n\n"
            content += f"**ğŸ“„ æ ‡é¢˜ï¼š** {u.get('title','')}\n\n"
            if u.get('summary'):
                content += f"**ğŸ“ æ‘˜è¦ï¼š**\n\n{u['summary']}\n\n"
            if u.get('outline'):
                content += f"**ğŸ“š å¤§çº²ï¼š**\n\n{u['outline']}\n\n"
            if u.get('url'):
                content += f"**ğŸ”— åŸæ–‡ï¼š** [è®¿é—®åŸç½‘ç«™]({u['url']})\n\n"
            if u.get('links'):
                content += f"**ğŸ”— ç›¸å…³é“¾æ¥ï¼š**\n"
                for link in u['links'][:5]:
                    content += f"- [{link.get('title','é“¾æ¥')}]({link.get('link','')})\n"
                content += "\n"
            content += "---\n\n"
            
        return self.send_markdown("åšä¸»æ›´æ–°æ¨é€", content, at_all, at_mobiles)

class BloggerMonitor:
    """åšä¸»ç›‘æ§ä¸»ç±»"""
    def __init__(self, config_file: str = "config.json"):
        self.config = self._load_config(config_file)
        self.extractor = ContentExtractor()
        self.db = DatabaseManager()
        self.notifier = NotificationManager(self.config.get('notification', {}))
        self.bloggers = []
        # AIå¤„ç†å™¨ï¼ˆå¯é€‰ï¼‰
        ai_key = self.config.get('deepseek_api_key') or self.config.get('openai_api_key')
        ai_base = self.config.get('ai_base_url', 'https://api.deepseek.com')
        ai_model = self.config.get('ai_model', 'deepseek-chat')
        self.ai_processor = AIContentProcessor(ai_key, ai_base, ai_model) if ai_key else None
        # åˆå§‹åŒ–é’‰é’‰æ¨é€å™¨ï¼ˆæ–°å¢ï¼‰
        dt_cfg = self.config.get('dingtalk', {})
        self.dingtalk_notifier = None
        if dt_cfg.get('enabled') and dt_cfg.get('webhook_url'):
            self.dingtalk_notifier = DingTalkNotifier(
                webhook_url=dt_cfg['webhook_url'],
                secret=dt_cfg.get('secret')
            )
    
    def _load_config(self, config_file: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            "check_interval_hours": 6,
            "notification": {
                "methods": ["dingtalk", "file"],  # é»˜è®¤é’‰é’‰+æ–‡ä»¶
                "smtp_server": "smtp.gmail.com",
                "smtp_port": 587,
                "sender_email": "",
                "sender_password": "",
                "recipient_email": ""
            },
            # AIå¯é€‰é…ç½®
            "deepseek_api_key": "",
            "ai_base_url": "https://api.deepseek.com",
            "ai_model": "deepseek-chat",
            # é’‰é’‰æ¨é€ï¼ˆæ–°å¢ï¼‰
            "dingtalk": {
                "enabled": True,
                "webhook_url": "",
                "secret": "",
                "at_all": False,
                "at_mobiles": []
            }
        }
        
        if os.path.exists(config_file):
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    default_config.update(config)
            except Exception as e:
                logging.warning(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
        
        return default_config
    
    def load_bloggers_from_excel(self, excel_file: str = "æµ·å¤–åšä¸».xlsx"):
        """ä»Excelæ–‡ä»¶åŠ è½½åšä¸»ä¿¡æ¯"""
        try:
            df = pd.read_excel(excel_file)
            
            for _, row in df.iterrows():
                if pd.notna(row['ç½‘å€']):
                    blogger = BloggerInfo(
                        name=row['å§“å'] if pd.notna(row['å§“å']) else "æœªçŸ¥",
                        url=row['ç½‘å€'],
                        description=row['èº«ä»½ / ç®€ä»‹'] if pd.notna(row['èº«ä»½ / ç®€ä»‹']) else "",
                        platform=row['ä¸»è¦åˆ†äº«å¹³å°'] if pd.notna(row['ä¸»è¦åˆ†äº«å¹³å°']) else ""
                    )
                    
                    # æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å·²å­˜åœ¨
                    existing = self.db.get_blogger_by_url(blogger.url)
                    if existing:
                        blogger.last_hash = existing.last_hash
                        blogger.last_check = existing.last_check
                        blogger.last_update = existing.last_update
                    
                    self.bloggers.append(blogger)
            
            logging.info(f"æˆåŠŸåŠ è½½ {len(self.bloggers)} ä¸ªåšä¸»ä¿¡æ¯")
            
        except Exception as e:
            logging.error(f"åŠ è½½Excelæ–‡ä»¶å¤±è´¥: {e}")
    
    def check_all_updates(self) -> List[Dict]:
        """æ£€æŸ¥æ‰€æœ‰åšä¸»çš„æ›´æ–°"""
        updates = []
        for blogger in self.bloggers:
            try:
                logging.info(f"æ£€æŸ¥ {blogger.name} çš„æ›´æ–°...")
                current_content = self.extractor.extract_content(blogger.url)
                current_hash = current_content.get('hash', '')
                blogger.last_check = datetime.now().isoformat()

                if current_hash != blogger.last_hash and current_hash:
                    logging.info(f"{blogger.name} æœ‰æ–°æ›´æ–°ï¼")
                    blogger.last_hash = current_hash
                    blogger.last_update = datetime.now().isoformat()
                    blogger_id = self.db.save_blogger(blogger)
                    self.db.save_update(
                        blogger_id,
                        current_content.get('title', ''),
                        current_content.get('content', ''),
                        current_content.get('links', []),
                        current_hash
                    )
                    # ç”ŸæˆAIæ‘˜è¦ï¼ˆåŸºäºæ ‡é¢˜ + å†…å®¹ç‰‡æ®µï¼‰
                    summary, outline = "", ""
                    if self.ai_processor:
                        ai_res = self.ai_processor.generate_summary_and_outline(
                            current_content.get('title', ''),
                            current_content.get('content', '')
                        )
                        summary = ai_res.get('summary', '')
                        outline = ai_res.get('outline', '')

                    updates.append({
                        'blogger_name': blogger.name,
                        'url': blogger.url,
                        'title': current_content.get('title', ''),
                        'content': current_content.get('content', ''),
                        'links': current_content.get('links', []),
                        'update_time': blogger.last_update,
                        'summary': summary,
                        'outline': outline
                    })
                else:
                    self.db.save_blogger(blogger)

                time.sleep(2)
            except Exception as e:
                logging.error(f"æ£€æŸ¥ {blogger.name} æ—¶å‡ºé”™: {e}")
        return updates
    
    def send_notifications(self, updates: List[Dict]):
        """å‘é€é€šçŸ¥"""
        if not updates:
            logging.info("æ²¡æœ‰æ–°çš„æ›´æ–°ï¼Œæ— éœ€å‘é€é€šçŸ¥")
            return
        
        methods = self.config.get('notification', {}).get('methods', ['file'])
        
        if 'dingtalk' in methods and self.dingtalk_notifier:
            dt_cfg = self.config.get('dingtalk', {})
            self.dingtalk_notifier.send_message(
                updates,
                at_all=dt_cfg.get('at_all', False),
                at_mobiles=dt_cfg.get('at_mobiles', [])
            )
        
        if 'email' in methods:
            self.notifier.send_email_notification(updates)
        
        if 'file' in methods:
            self.notifier.save_to_file(updates)
    
    def run_check(self):
        """æ‰§è¡Œä¸€æ¬¡æ£€æŸ¥"""
        logging.info("å¼€å§‹æ£€æŸ¥åšä¸»æ›´æ–°...")
        updates = self.check_all_updates()
        
        if updates:
            logging.info(f"å‘ç° {len(updates)} ä¸ªæ›´æ–°")
            self.send_notifications(updates)
        else:
            logging.info("æ²¡æœ‰å‘ç°æ–°çš„æ›´æ–°")
    
    def start_monitoring(self):
        """å¼€å§‹å®šæœŸç›‘æ§"""
        # åŠ è½½åšä¸»ä¿¡æ¯
        self.load_bloggers_from_excel()
        
        if not self.bloggers:
            logging.error("æ²¡æœ‰åŠ è½½åˆ°åšä¸»ä¿¡æ¯ï¼Œæ— æ³•å¼€å§‹ç›‘æ§")
            return
        
        # è®¾ç½®å®šæœŸä»»åŠ¡
        interval_hours = self.config.get('check_interval_hours', 6)
        schedule.every(interval_hours).hours.do(self.run_check)
        
        # ç«‹å³æ‰§è¡Œä¸€æ¬¡æ£€æŸ¥
        self.run_check()
        
        logging.info(f"å¼€å§‹å®šæœŸç›‘æ§ï¼Œæ£€æŸ¥é—´éš”: {interval_hours} å°æ—¶")
        
        # æŒç»­è¿è¡Œ
        while True:
            schedule.run_pending()
            time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡æ˜¯å¦éœ€è¦æ‰§è¡Œä»»åŠ¡

def main():
    """ä¸»å‡½æ•°"""
    try:
        # å¯ä»¥é€‰æ‹©è¿è¡Œæ¨¡å¼
        import sys
        
        if len(sys.argv) > 1 and sys.argv[1] == '--fix-db':
            # å¼ºåˆ¶ä¿®å¤æ•°æ®åº“
            print("æ­£åœ¨ä¿®å¤æ•°æ®åº“ç»“æ„...")
            db = DatabaseManager()
            print("âœ… æ•°æ®åº“ä¿®å¤å®Œæˆ")
            return
        
        print("æ­£åœ¨åˆå§‹åŒ– BloggerMonitor...")
        monitor = BloggerMonitor()
        print("âœ… BloggerMonitor åˆå§‹åŒ–æˆåŠŸ")
        
        if len(sys.argv) > 1 and sys.argv[1] == '--once':
            # åªè¿è¡Œä¸€æ¬¡æ£€æŸ¥
            print("å¼€å§‹åŠ è½½åšä¸»ä¿¡æ¯...")
            monitor.load_bloggers_from_excel()
            print("âœ… åšä¸»ä¿¡æ¯åŠ è½½å®Œæˆ")
            
            print("å¼€å§‹æ£€æŸ¥æ›´æ–°...")
            monitor.run_check()
            print("âœ… æ£€æŸ¥å®Œæˆ")
        else:
            # æŒç»­ç›‘æ§æ¨¡å¼
            monitor.start_monitoring()
            
    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()